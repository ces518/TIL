# 토스 데이터의 흐름과 활용
- Sqoop -> HDFS 로 적재
- 

## 데이터 정의
- 테이블 및 로그 정의
- MySQL
- MongoDB 
- Kubernates

## 데이터 수집 / 저장
- MySQL -> Sqoop 을 통해 HDFS 로 적재
- MongoDB -> hive mongo storage handler 를 통해 HDFS 로 적재
- Kafka 에 적재된 로그 -> HBase, Kudu, HDFS 에 적재

> Kafka 를 세가지로 운영중 \n
> 서비스 큐 / 로그 큐 / 데이터 플랫폼
> 클러스터간의 데이터 미러링, HDFS / Kudu / HBase / InfluxDB  다양한 포맷에 맞게 스트리핑 애플리케이션을 활용하여 적재
> 2개의 데이터 센터에 active-active 구조로 운영

- Hadoop 에 적재된 데이터는 데이터 노드 스토리지 영역을 구분하여 사용
    - HDFS 의 Heterogeneous Storage 기능 활용
    - SSD / DISK / NAS 등에 적재
    - HOT , WARM , COLD 영역을 나누어 적재
    
## 추출 가공 적재
- HDFS / Kudu 에 저장된 데이터 -> Impala, Hive, Spark 를 이용해 처리
- Jenkins, Airflow 를 이용하여 배치 플로우 관리

### Impala
- Multi-User Performance
- Query Throughput
- Spark, Hive 보다 일반적인 케이스에서 성능이 좋음

> EDA (Exploratory data analysis) , ETL(Extraction, Transformation, Loading) 시 Impala 를 주로 활용

- Catalog 서버 / StateStore 서버 / Impala 데몬들도 구성되어 있다.
- Catalog 서버 -> 메타 데이터를 캐시
- StateStore 서버 -> Impala 데몬 상태 모니터링과 메타 데이터를 브로드캐스팅
- Impala 데몬 -> 실제 데이터를 R/W , 쿼리 병렬과 처리 노드를 분산
    - coordinator 와 executor 로 역할 구분
    - coordinator -> JDBC/ODBC 를 통해 쿼리를 listen

`한계점`
- Graceful Shutdown / Retry 미제공, Impala 노드 간 메타데이터 SYNC 가 필요
- MR, SPARK, Hive 처리시 Impala 로 메타정보가 반영되지 않는다.
- Catalog, StateStore - SPOF (H/A 지원하지 않는다)
- 메모리 문제 때문에 컴포넌트간 리소스 간섭 (실제 메모리보다 크게 잡아야 한다)
- 메트릭 모니터링 및 운영이 어려움
    - 클러스터 통합 메트릭을 제공하지 않는다.
    
## 분석
- Jupyter Hue  

## 모니터링
- Kubernates 로 구동중인 애플리케이션 -> 프로메테우스
- 그외 서비스 관련 -> Druid, InfluxDB, ES 등을 활용해 Kibana, Grafana 사용

## 활용
- 실시간 데이터 -> 스트리밍 처리를 통해 Redis, HBase, MongoDB
- 배치 데이터 -> MySQL, MongoDB